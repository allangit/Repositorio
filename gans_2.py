# -*- coding: utf-8 -*-
"""Gans_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N97hfw4Zh8JnDBeHUocurUZsm8EcLI6e
"""

import pandas as pd
import numpy as np
import cv2
import tensorflow as tf
import random
import matplotlib.pyplot as plt
from keras.datasets import mnist,fashion_mnist
from keras.layers import Dense,MaxPooling2D,MaxPool2D,Dropout,Flatten,Input,BatchNormalization,LeakyReLU,Conv2D,Conv2DTranspose,Reshape
from keras.models import Sequential,Model
from google.colab.patches import cv2_imshow

(xtrain,ytrain),(xtest,ytest)=tf.keras.datasets.mnist.load_data()

print(xtrain.shape)
print(xtest.shape)

i = np.random.randint(0, 60000)
#print(i)
print(ytrain[i])
plt.imshow(xtrain[i], cmap='gray');

xtrain = (xtrain - 127.5) / 127.5

print(f'MAX= {xtrain.max()} Min= {xtrain.min()}')

#plot
i=np.random.randint(0,59999)
print(i)
plt.imshow(xtrain[i],cmap='gray')

#formateo de las imagenes
xtrain=xtrain.reshape(xtrain.shape[0],28,28,1).astype('float32')
xtest=xtest.reshape(xtest.shape[0],28,28,1).astype('float32')
print(f'{xtrain.shape} , {xtest.shape}')

buffer_size=60000
batch_size=256
type(xtrain)

#formato a data tensorflow
xtrain=tf.data.Dataset.from_tensor_slices(xtrain).shuffle(buffer_size).batch(batch_size)
print(xtrain)

def build_generator():
  network = Sequential()

  network.add(Dense(units = 7*7*256, use_bias = False, input_shape=(100,)))
  network.add(BatchNormalization())
  network.add(LeakyReLU())

  network.add(Reshape((7,7,256)))

  # 7x7x128
  network.add(Conv2DTranspose(filters = 128, kernel_size = (5,5), padding = 'same', use_bias = False))
  network.add(BatchNormalization())
  network.add(LeakyReLU())

  # 14x14x64
  network.add(Conv2DTranspose(filters = 64, kernel_size = (5,5), padding = 'same', strides = (2,2), use_bias = False))
  network.add(BatchNormalization())
  network.add(LeakyReLU())

  # 28x28x1
  network.add(Conv2DTranspose(filters = 1, kernel_size = (5,5), padding = 'same', strides = (2,2), use_bias=True, activation='tanh'))

  network.summary()

  return network

generator=build_generator()

generator.input

noise=tf.random.normal([1,100])
noise

generated_image=generator(noise,training=False)

generated_image.shape

generated_image[0,:,:,0].shape

plt.imshow(generated_image[0,:,:,0],cmap='gray')

def build_discriminator():
  network = Sequential()

  # 14x14x64
  network.add(Conv2D(filters = 64, strides = (2,2), kernel_size = (5,5), padding = 'same', input_shape = [28,28,1]))
  network.add(LeakyReLU())
  network.add(Dropout(0.3))

  # 7x7x128
  network.add(Conv2D(filters = 128, strides = (2,2), kernel_size = (5,5), padding = 'same'))
  network.add(LeakyReLU())
  network.add(Dropout(0.3))

  network.add(Flatten())
  network.add(Dense(1))

  network.summary()

  return network

discriminator=build_discriminator()

discriminator.input

discriminator(generated_image,training=False)

cross_entropy=tf.keras.losses.BinaryCrossentropy(from_logits=True)

def discriminator_loss(expected_output,fake_output):

  real_loss=cross_entropy(tf.ones_like(expected_output),expected_output)
  fake_loss=cross_entropy(tf.zeros_like(fake_output),fake_output)
  total_loss=real_loss+fake_output

  return total_loss

def generator_loss(fake_output):

  return cross_entropy(tf.ones_like(fake_output),fake_output)

#optimizadores
generator_optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001)
discriminator_optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001)

epochs=100
noise_dimension=100
number_of_images=16

batch_size,noise_dimension

@tf.function
def train(images):
  noise = tf.random.normal([batch_size, noise_dimension])
  #print(noise.shape)
  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
    generated_images = generator(noise, training = True)

    expected_output = discriminator(images, training = True)
    fake_output = discriminator(generated_images, training = True)

    gen_loss = generator_loss(fake_output)
    disc_loss = discriminator_loss(expected_output, fake_output)

  generator_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)
  discriminator_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

  generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))
  discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))

xtrain

def train_gan(dataset, epochs, test_images):
  for epoch in range(epochs):
    #print(epoch)
    for image_batch in dataset:
      #print(image_batch.shape)
      train(image_batch)

    print('Epoch: ', epoch + 1)
    generated_images = generator(test_images, training = False)
    fig = plt.figure(figsize = (10,10))
    for i in range(generated_images.shape[0]):
      plt.subplot(4,4,i + 1)
      plt.imshow(generated_images[i, :, :, 0] * 127.5 + 127.5, cmap='gray')
      plt.axis('off')
    plt.show()

test_images = tf.random.normal([number_of_images, noise_dimension])

train_gan(xtrain, epochs, test_images)